<TeXmacs|2.1>

<style|article>

<\body>
  <doc-data|<doc-title|A Fast Optimizer with Momentum and Sign>>

  <center|Peng-xu Jiang<\footnote>
    Email: shuiruge@whu.edu.cn
  </footnote>>

  <abstract-data|<abstract|We propose a fast optimiztion algorithm for deep
  learning that uses only the sign of momentum. This optimizer is as fast as
  Adam, but occupies half of the memory that Adam does. We also give an
  analysis of the effect of decay factor, which is the only hyper-parameter
  except for learning rate. We validate the analysis on the fashion-MNIST
  dataset.>>

  <section|Backgroud>

  <subsection|Gradient Descent Method>

  For minimizing a smooth function <math|h:\<bbb-R\><rsup|n>\<rightarrow\>\<bbb-R\>>,
  standard gradient descent method computes the gradient of <math|h>, and
  iterates along the negative direction of gradient so as to decrease
  <math|h> at each iteration. Explicitly, let <math|t\<in\>\<bbb-N\>> denotes
  the step of iteration, thus the variable at step <math|t+1> is given by

  <\equation>
    x<rsub|t+1>=x<rsub|t>-\<eta\> \<nabla\>L<around*|(|x<rsub|t>|)>,<label|equation:gradient
    descent method>
  </equation>

  where the <math|\<eta\>> is the learning rate. Since <math|h> is smooth, we
  have <math|h<around*|(|x<rsub|t+1>|)>\<less\>h<around*|(|x<rsub|t>|)>> as
  long as the learning rate is sufficiently small and
  <math|\<nabla\>h<around*|(|x<rsub|t>|)>\<neq\>0>. Thus, the iteration
  (<reference|equation:gradient descent method>) always decrease <math|h>
  until the (maybe local) minimum.

  Problems arise when applying gradient descent method directly to minimize
  <math|L<rsub|D>> because of the random disturbance
  <math|\<delta\>L<rsub|D>>. What we really want to minimize is the
  deterministic <math|<wide|L|^><rsub|D>>, the loss function on the
  full-batch, but what we can obtain is the <math|L<rsub|D>> instead of
  <math|<wide|L|^><rsub|D>>. We hope that, iterated by the gradient descent
  method (<reference|equation:gradient descent method>), the trajectory
  <math|<around*|(|\<theta\><rsub|0>,\<theta\><rsub|1>,\<ldots\>|)>>
  generated by <math|\<nabla\>L<rsub|D>> (what we can compute) and the
  <math|<around*|(|<wide|\<theta\>|^><rsub|0>,<wide|\<theta\>|^><rsub|1>,\<ldots\>|)>>
  by <math|\<nabla\><wide|L|^><rsub|D>> (what we expect to compute but
  cannot) share the same limit <math|\<theta\><rsub|\<star\>>>, the real
  best-fit value. Only when <math|\<nabla\>L<rsub|D><around*|(|\<theta\>|)>>
  is sufficiently close to <math|\<nabla\><wide|L|^><rsub|D><around*|(|\<theta\>|)>>
  can this be done, which indicates that we have to reduce the randomness
  from <math|\<nabla\>\<delta\>L<rsub|D>>.

  <subsection|Momentum>

  <subsection|Sign of Gradient>

  <section|Method>

  <section|Experiments and Results>

  <section|Conclusion>

  \;
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-2|<tuple|1.1|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-3|<tuple|1.2|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-4|<tuple|1.3|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-5|<tuple|2|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-6|<tuple|3|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|auto-7|<tuple|4|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|equation:gradient descent method|<tuple|1|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|footnote-1|<tuple|1|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
    <associate|footnr-1|<tuple|1|?|../../.TeXmacs/texts/scratch/no_name_4.tm>>
  </collection>
</references>